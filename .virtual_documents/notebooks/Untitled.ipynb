import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras.applications.mobilenet_v2 import preprocess_input
from tensorflow.keras.utils import image_dataset_from_directory
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
import matplotlib.pyplot as plt
import os



dataset_path = "../data/processed/train_split"

train_ds = image_dataset_from_directory(
    dataset_path,
    validation_split=0.2,
    subset="training",
    seed=42,
    image_size=(96, 96),   # ðŸ”¥ Upgraded size
    batch_size=32
)

val_ds = image_dataset_from_directory(
    dataset_path,
    validation_split=0.2,
    subset="validation",
    seed=42,
    image_size=(96, 96),
    batch_size=32
)

class_names = train_ds.class_names
print("Classes:", class_names)



data_augmentation = tf.keras.Sequential([
    tf.keras.layers.Input(shape=(96, 96, 3)),
    tf.keras.layers.RandomFlip("horizontal"),
    tf.keras.layers.RandomRotation(0.1),
    tf.keras.layers.RandomZoom(0.1),
])



# Use MobileNetV2's official preprocessing function
preprocess_layer = tf.keras.layers.Lambda(preprocess_input)



AUTOTUNE = tf.data.AUTOTUNE
train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)
val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)



base_model = MobileNetV2(
    input_shape=(96, 96, 3),
    include_top=False,
    weights='imagenet'
)
base_model.trainable = False  # Freeze the pretrained backbone

# Build the transfer learning model
inputs = tf.keras.Input(shape=(96, 96, 3))
x = data_augmentation(inputs)
x = preprocess_layer(x)
x = base_model(x, training=False)
x = layers.GlobalAveragePooling2D()(x)
x = layers.Dropout(0.3)(x)
x = layers.Dense(128, activation='relu')(x)
x = layers.Dropout(0.2)(x)
outputs = layers.Dense(10, activation='softmax')(x)

model = tf.keras.Model(inputs, outputs)
model.summary()



model.compile(
    optimizer=tf.keras.optimizers.Adam(),
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)



early_stop = EarlyStopping(
    monitor='val_accuracy',
    patience=5,
    restore_best_weights=True
)

lr_schedule = ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.5,
    patience=3,
    verbose=1
)

callbacks = [early_stop, lr_schedule]



history = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=30,
    callbacks=callbacks
)



# Unfreeze the base model
base_model.trainable = True

# Freeze only early layers to avoid overfitting
fine_tune_at = 100  # You can adjust this
for layer in base_model.layers[:fine_tune_at]:
    layer.trainable = False

print(f"âœ… Unfroze from layer {fine_tune_at} onwards")



model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),  # ðŸ”¥ Low LR!
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)



history_finetune = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=15,
    callbacks=callbacks
)



# Combine accuracy and loss from both training phases
acc = history.history['accuracy'] + history_finetune.history['accuracy']
val_acc = history.history['val_accuracy'] + history_finetune.history['val_accuracy']
loss = history.history['loss'] + history_finetune.history['loss']
val_loss = history.history['val_loss'] + history_finetune.history['val_loss']
epochs_range = range(len(acc))

# Plot
plt.figure(figsize=(14, 5))

plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, label='Training Accuracy')
plt.plot(epochs_range, val_acc, label='Validation Accuracy')
plt.title('Full Training: Accuracy')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label='Training Loss')
plt.plot(epochs_range, val_loss, label='Validation Loss')
plt.title('Full Training: Loss')
plt.legend()

plt.show()



from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import numpy as np

# Get true labels and predictions from validation set
y_true = []
y_pred = []

for images, labels in val_ds.unbatch():
    pred = model.predict(tf.expand_dims(images, axis=0), verbose=0)
    y_true.append(labels.numpy())
    y_pred.append(np.argmax(pred))

# Compute and display confusion matrix
cm = confusion_matrix(y_true, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)

plt.figure(figsize=(10, 8))
disp.plot(cmap='Blues', xticks_rotation=45)
plt.title("Confusion Matrix on Validation Set")
plt.show()



# Find misclassified examples
misclassified_images = []
true_labels = []
pred_labels = []

for images, labels in val_ds.unbatch().take(500):  # adjust limit if needed
    pred = model.predict(tf.expand_dims(images, axis=0), verbose=0)
    predicted_label = np.argmax(pred)
    true_label = labels.numpy()

    if predicted_label != true_label:
        misclassified_images.append(images.numpy())
        true_labels.append(true_label)
        pred_labels.append(predicted_label)

    if len(misclassified_images) == 9:
        break

# Plot them
plt.figure(figsize=(10, 10))
for i in range(9):
    ax = plt.subplot(3, 3, i + 1)
    plt.imshow(misclassified_images[i])
    true_class = class_names[true_labels[i]]
    pred_class = class_names[pred_labels[i]]
    plt.title(f"True: {true_class}\nPred: {pred_class}")
    plt.axis("off")



acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']
epochs_range = range(len(acc))

plt.figure(figsize=(14, 5))
plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, label='Training Accuracy')
plt.plot(epochs_range, val_acc, label='Validation Accuracy')
plt.title('Training and Validation Accuracy')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label='Training Loss')
plt.plot(epochs_range, val_loss, label='Validation Loss')
plt.title('Training and Validation Loss')
plt.legend()

plt.show()



# Make sure models/ directory exists
os.makedirs("../models", exist_ok=True)

# Save the fine-tuned model
model.save("../models/mobilenetv2_finetuned.keras")

print("âœ… Fine-tuned model saved successfully!")





